{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ffa2668",
   "metadata": {},
   "source": [
    "# Streaming application using Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cd32ec",
   "metadata": {},
   "source": [
    "### 1. Write code to create a SparkSession, which uses four cores with a proper application name, use the Melbourne timezone, and make sure a checkpoint location has been set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46e7d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6fe1496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.4.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0 pyspark-shell'\n",
    "spark_conf = SparkConf().setMaster(\"local[4]\").setAppName(\"A2B_rsin0045\")\n",
    "spark = SparkSession.builder.config(conf=spark_conf).config(\"spark.sql.session.timeZone\",\"Australia/Melbourne\").config(\"spark.sql.streaming.checkpointLocation\",'checkpt_folder').config(\"spark.sql.streaming.statefulOperator.checkCorrectness.enabled\", \"false\").getOrCreate()\n",
    "sc = spark.sparkContext.setLogLevel('ERROR')\n",
    "print(\"Spark timezone:\",spark.conf.get(\"spark.sql.session.timeZone\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f6b739",
   "metadata": {},
   "source": [
    "### 2. Similar to assignment 2A, write code to define the data schema for the data files, following the data types suggested in the metadata file. Load the static datasets (e.g. customer, product, category) into data frames. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4205bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType,DoubleType\n",
    "\n",
    "category_schema = StructType([\n",
    "    StructField(\"del\", StringType(), True),\n",
    "    StructField(\"category_id\", IntegerType(), True),\n",
    "    StructField(\"cat_level1\", StringType(), True),\n",
    "    StructField(\"cat_level2\", StringType(), True),\n",
    "    StructField(\"cat_level3\", StringType(), True)\n",
    "])\n",
    "\n",
    "customer_schema = StructType([\n",
    "    StructField(\"del\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"username\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"birthdate\", StringType(), True), \n",
    "    StructField(\"device_type\", StringType(), True),\n",
    "    StructField(\"device_id\", StringType(), True),\n",
    "    StructField(\"device_version\", StringType(), True),\n",
    "    StructField(\"home_location_lat\", FloatType(), True),\n",
    "    StructField(\"home_location_long\", FloatType(), True),\n",
    "    StructField(\"home_location\", StringType(), True),\n",
    "    StructField(\"home_country\", StringType(), True),\n",
    "    StructField(\"first_join_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "product_schema = StructType([\n",
    "    StructField(\"del\", StringType(), True),\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"baseColour\", StringType(), True),\n",
    "    StructField(\"season\", StringType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"usage\", StringType(), True),\n",
    "    StructField(\"productDisplayName\", StringType(), True),\n",
    "    StructField(\"category_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "click_stream_schema = StructType([\n",
    "    StructField(\"del\", StringType(), True),\n",
    "    StructField(\"session_id\", StringType(), True),\n",
    "    StructField(\"event_name\", StringType(), True),\n",
    "    StructField(\"event_time\", TimestampType(), True),\n",
    "    StructField(\"event_id\", StringType(), True),\n",
    "    StructField(\"traffic_source\", StringType(), True),\n",
    "    StructField(\"event_metadata\", StringType(), True)\n",
    "])\n",
    "\n",
    "transaction_schema = StructType([\n",
    "    StructField(\"del\", StringType(), True),\n",
    "    StructField(\"created_at\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"booking_id\", StringType(), True),\n",
    "    StructField(\"session_id\", StringType(), True),\n",
    "    StructField(\"product_metadata\", StringType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"payment_status\", StringType(), True),\n",
    "    StructField(\"promo_amount\", FloatType(), True),\n",
    "    StructField(\"promo_code\", StringType(), True),\n",
    "    StructField(\"shipment_fee\", FloatType(), True),\n",
    "    StructField(\"shipment_date_limit\", StringType(), True),\n",
    "    StructField(\"shipment_location_lat\", FloatType(), True),\n",
    "    StructField(\"shipment_location_long\", FloatType(), True),\n",
    "    StructField(\"total_amount\", FloatType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7b6d50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_df = spark.read.csv(\"category.csv\", header=True, schema=category_schema)\n",
    "customer_df = spark.read.csv(\"customer.csv\", header=True, schema=customer_schema)\n",
    "product_df = spark.read.csv(\"product.csv\", header=True, schema=product_schema)\n",
    "click_stream_df = spark.read.csv(\"click_stream.csv\", header=True, schema=click_stream_schema)\n",
    "transaction_df = spark.read.csv(\"new_transactions.csv\", header=True, schema=transaction_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d8378f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_df = category_df.drop(\"del\")\n",
    "customer_df = customer_df.drop(\"del\")\n",
    "product_df = product_df.drop(\"del\")\n",
    "click_stream_df = click_stream_df.drop(\"del\")\n",
    "transaction_df = transaction_df.drop(\"del\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0214d7f0",
   "metadata": {},
   "source": [
    "### 3 Using the Kafka topic from the producer in Task 1, ingest the streaming data into Spark Streaming, assuming all data comes in the String format. Except for the 'ts' column, you shall receive it as an Int type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4f3029ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamdf = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", f'{\"192.168.0.140\"}:9092') \\\n",
    "  .option(\"subscribe\", \"Stream_Data_MOTH\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "45c39cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cb24b768",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamdf = streamdf.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "streamdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fec42f",
   "metadata": {},
   "source": [
    "### 4 Then, the streaming data format should be transformed into the proper formats following the metadata file schema, similar to assignment 2A.  \n",
    "Perform the following tasks:  \n",
    "a) For the 'ts' column, convert it to the timestamp format, we will use it as event_time.  \n",
    "b) If the data is late for more than 1 minute, discard it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db14e6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType,IntegerType,TimestampType\n",
    "schema = ArrayType(StructType([    \n",
    "    StructField('#', StringType(), True), \n",
    "    StructField('session_id', StringType(), True),\n",
    "    StructField('event_name', StringType(), True), \n",
    "    StructField('event_id', StringType(), True),\n",
    "    StructField('traffic_source', StringType(), True), \n",
    "    StructField('event_metadata', StringType(), True),\n",
    "    StructField('customer_id', StringType(), True),\n",
    "    StructField('ts', TimestampType(), True)\n",
    "]))\n",
    "streamdf = streamdf.select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias('parsed_value'))\n",
    "streamdff = streamdf.select(F.explode(F.col(\"parsed_value\")).alias('unnested_value'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5cfd2c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamdf = streamdff.select(\n",
    "                    F.col(\"unnested_value.session_id\").alias(\"session_id\"),\n",
    "                    F.col(\"unnested_value.event_name\").alias(\"event_name\"),\n",
    "                    F.col(\"unnested_value.event_id\").alias(\"event_id\"),\n",
    "                    F.col(\"unnested_value.traffic_source\").alias(\"traffic_source\"),\n",
    "                    F.col(\"unnested_value.event_metadata\").alias(\"event_metadata\"),\n",
    "                    F.col(\"unnested_value.customer_id\").alias(\"customer_id\"),\n",
    "                    F.col(\"unnested_value.ts\").alias(\"event_time\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8ef36d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "q11 = streamdf \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .option(\"truncate\",\"false\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5303c19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q11.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "057af630",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamdf = streamdf.withWatermark(\"event_time\",\"1 minute\")\n",
    "streamdf = streamdf.filter(streamdf[\"event_time\"] >= F.expr(\"event_time - interval 1 minute\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "025ad001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, month\n",
    "\n",
    "category_conditions = {\n",
    "    \"add_promo\": (col(\"event_name\") == \"ADD_PROMO\"),\n",
    "    \"num_cat_highvalue\": (col(\"event_name\") == \"ADD_PROMO\") | (col(\"event_name\") == \"ADD_TO_CART\"),\n",
    "    \"num_cat_midvalue\": (col(\"event_name\") == \"VIEW_PROMO\") | (col(\"event_name\") == \"VIEW_ITEM\") | (col(\"event_name\") == \"SEARCH\"),\n",
    "    \"num_cat_lowvalue\": (col(\"event_name\") == \"SCROLL\") | (col(\"event_name\") == \"HOMEPAGE\") | (col(\"event_name\") == \"CLICK\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a7d9ed38",
   "metadata": {},
   "outputs": [],
   "source": [
    "click_stream_df = click_stream_df.withColumn(\"event_month\", month(col(\"event_time\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe817b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "for category_name, condition in category_conditions.items():\n",
    "    click_stream_df = click_stream_df.withColumn(category_name, when(condition, 1).otherwise(0))\n",
    "\n",
    "# Group by session_id and sum the values for each category column\n",
    "feature_df = click_stream_df.groupBy(\"session_id\").agg(\n",
    "    F.sum(\"add_promo\").alias(\"add_promo\"),\n",
    "    F.sum(\"num_cat_highvalue\").alias(\"num_cat_highvalue\"),\n",
    "    F.sum(\"num_cat_midvalue\").alias(\"num_cat_midvalue\"),\n",
    "    F.sum(\"num_cat_lowvalue\").alias(\"num_cat_lowvalue\"),\n",
    "    F.max(\"event_month\").alias(\"event_month\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d6c8e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of actions (total_actions) for each session\n",
    "feature_df = feature_df.withColumn(\"total_actions\", col(\"num_cat_highvalue\") + col(\"num_cat_midvalue\") + col(\"num_cat_lowvalue\"))\n",
    "\n",
    "# Calculate the percentage ratio of high-value actions\n",
    "feature_df = feature_df.withColumn(\"high_value_ratio\", (col(\"num_cat_highvalue\") / col(\"total_actions\")) * 100)\n",
    "\n",
    "# Calculate the percentage ratio of low-value actions\n",
    "feature_df = feature_df.withColumn(\"low_value_ratio\", (col(\"num_cat_lowvalue\") / col(\"total_actions\")) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b78c864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = feature_df.withColumn(\"is_promotion\", when(col(\"add_promo\") > 0, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "08720d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, month, when,udf\n",
    "def map_to_season1(month):\n",
    "    if month in [3, 4, 5]:\n",
    "        return \"Spring\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"Summer\"\n",
    "    elif month in [9, 10, 11]:\n",
    "        return \"Autumn\"\n",
    "    else:\n",
    "        return \"Winter\"\n",
    "map_to_season = udf(lambda z: map_to_season1(z))\n",
    "feature_df = feature_df.withColumn(\"season\", when(col(\"event_month\").isNotNull(), map_to_season(col(\"event_month\"))).otherwise(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d78054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year,current_date\n",
    "joined_df1 = feature_df.join(transaction_df, \"session_id\", \"inner\")\n",
    "joined_df = joined_df1.join(customer_df, \"customer_id\", \"inner\")\n",
    "joined_df = joined_df.withColumn(\"age\", year(current_date()) - year(col(\"birthdate\")))\n",
    "joined_df = joined_df.withColumn(\"first_join_year\", year(col(\"first_join_date\")))\n",
    "customer_info_columns = [ \"gender\", \"device_type\", \"home_location\", \"first_join_year\",\"age\"]\n",
    "feature_df = joined_df.select(*(feature_df.columns + customer_info_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d2c52ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = feature_df.join(transaction_df, \"session_id\", \"inner\")\n",
    "joined_df = joined_df.withColumn(\"purchase_status\", when(col(\"payment_status\")  == \"Success\", 1).otherwise(0))\n",
    "feature_df = joined_df.select(\"session_id\", \"customer_id\",\"num_cat_highvalue\", \"num_cat_midvalue\", \"num_cat_lowvalue\", \"high_value_ratio\", \"low_value_ratio\", \"is_promotion\",\"season\", \"gender\", \"age\", \"device_type\", \"home_location\", \"first_join_year\", \"purchase_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5e2aceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features = feature_df.select( \"gender\", \"device_type\", \"home_location\", \"first_join_year\", \"age\",\"customer_id\",\"season\",'num_cat_highvalue', 'num_cat_lowvalue', 'num_cat_midvalue', 'high_value_ratio','is_promotion','low_value_ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ee233363",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamdf_df = streamdf.join(extracted_features,\"customer_id\",how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cec8e363",
   "metadata": {},
   "outputs": [],
   "source": [
    "q12 = streamdf_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .option(\"truncate\",\"false\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c551e243",
   "metadata": {},
   "outputs": [],
   "source": [
    "q12.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aa620238",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "streamdf_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13b38832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql.functions import window\n",
    "final_model = PipelineModel.load(\"better2\")\n",
    "pred = final_model.transform(streamdf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f30abf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_no = pred.filter(pred[\"prediction\"] == 1.0)\n",
    "windowsales = sales_no \\\n",
    "    .withWatermark(\"event_time\", \"10 seconds\") \\\n",
    "    .groupBy(window(\"event_time\", \"1 minute\", \"10 seconds\")).agg(F.sum(\"prediction\").alias(\"potential_sales\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20271619",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = windowsales \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .option(\"truncate\",\"false\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4b913975",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0843749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = pred.filter(pred[\"prediction\"] == 1.0)\n",
    "rev = rev.filter(rev[\"event_name\"] == \"ADD_TO_CART\")\n",
    "rev = rev.withColumn(\"item_price\",F.get_json_object(\"event_metadata\",'$.item_price'))\n",
    "rev_window = rev \\\n",
    "    .withWatermark(\"event_time\", \"30 seconds\") \\\n",
    "    .groupBy(window(\"event_time\", \"30 seconds\", \"30 seconds\")).agg(F.sum(\"item_price\").alias(\"potential_revenue\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6585d9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "q2 = rev_window \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime='30 seconds') \\\n",
    "    .option(\"truncate\",\"false\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8958748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "q2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8991573",
   "metadata": {},
   "outputs": [],
   "source": [
    "quan = pred.filter(pred[\"event_name\"] == \"ADD_TO_CART\")\n",
    "quan = quan.withColumn(\"id\",F.get_json_object(\"event_metadata\",'$.product_id'))\n",
    "quan = quan.withColumn(\"quantity\",F.get_json_object(\"event_metadata\",'$.quantity'))\n",
    "quan = quan.join(product_df,\"id\",how='left')\n",
    "quan_window = quan \\\n",
    "    .withWatermark(\"event_time\", \"30 seconds\") \\\n",
    "    .groupBy(window(\"event_time\", \"60 seconds\")).agg(F.sum(\"quantity\").alias(\"product_quantity\"),\\\n",
    "         F.max(\"productDisplayName\").alias(\"productDisplayName\")).sort(F.desc(\"product_quantity\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "43913992",
   "metadata": {},
   "outputs": [],
   "source": [
    "q3 = quan_window \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime='60 seconds') \\\n",
    "    .option(\"truncate\",\"false\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ec125de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "q3.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cdf5000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet= pred.select(\"prediction\",\"event_metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3db2554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_file_sink = df_parquet.writeStream.format(\"parquet\")\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .option(\"path\", \"parquet/clickstream_df\")\\\n",
    "        .option(\"checkpointLocation\", \"parquet/clickstream_df/checkpoint\")\\\n",
    "        .format(\"console\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a23bcb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_file_sink.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c686a9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"prediction\", DoubleType(), True),\n",
    "    StructField(\"event_metadata\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8d035c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- prediction: double (nullable = true)\n",
      " |-- event_metadata: string (nullable = true)\n",
      "\n",
      "+----------+--------------+\n",
      "|prediction|event_metadata|\n",
      "+----------+--------------+\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_file_sink_df = spark.read.schema(schema).parquet(\"parquet/clickstream_df\")\n",
    "query_file_sink_df.printSchema()\n",
    "query_file_sink_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f8eebd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qq1 = pred \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(processingTime='30 seconds') \\\n",
    "    .option(\"path\", \"parquet/clickstream_df\")\\\n",
    "    .option(\"checkpointLocation\", \"parquet/clickstream_df/checkpoint\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a762f8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "qq1.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "29317a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_json, struct\n",
    "kafka_broker = \"192.168.0.140:9092\"\n",
    "kafka_topic = \"predictions\"  \n",
    "schema = StructType([\n",
    "    StructField(\"prediction\", StringType(), True),\n",
    "    StructField(\"event_metadata\", StringType(), True)\n",
    "])\n",
    "query_file_sink_df = spark.readStream.schema(schema).parquet(\"parquet/clickstream_df\")\n",
    "query_file_sink_df = query_file_sink_df.withColumn(\"value\", to_json(struct(\"prediction\", \"event_metadata\")))\n",
    "query_file_sink_df = query_file_sink_df.select(\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2aeca52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_stream = query_file_sink_df \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"192.168.0.140:9092\") \\\n",
    "    .option(\"topic\", \"predictions\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f3346b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
